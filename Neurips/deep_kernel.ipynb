{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, PeriodicKernel, MaternKernel, CosineKernel\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor\n",
    "from scipy.stats import kurtosis,skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utilities.fits import fit\n",
    "# from utilities import plot\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from skgpytorch.metrics import mean_squared_error, negative_log_predictive_density\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# from datasets.dataset_load import dataset_loader\n",
    "from utilities import plot,fits,gmm,errors,predict,preprocess\n",
    "\n",
    "# device = \"cpu\"\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={1:{\n",
    "            'start_time': \"2011-04-28\" ,\n",
    "            'end_time': \"2011-05-15\"\n",
    "        }, \n",
    "        3: {\n",
    "        'start_time': \"2011-04-19\" ,\n",
    "        'end_time': \"2011-05-22\"\n",
    "        },\n",
    "            \n",
    "        }\n",
    "\n",
    "test = {  2: {\n",
    "                    'start_time': \"2011-04-21\" ,\n",
    "                    'end_time': \"2011-05-21\"\n",
    "                },\n",
    "                }\n",
    "\n",
    "# 6: {\n",
    "#                     'start_time': \"2011-05-25\" ,\n",
    "#                     'end_time': \"2011-06-13\"\n",
    "#                 }\n",
    "\n",
    "# 5: {\n",
    "#                     'start_time': \"2011-04-22\" ,\n",
    "#                     'end_time': \"2011-06-01\"\n",
    "#                 }\n",
    "appliances = [\"Microwave\", \"Refrigerator\",  \"Dish Washer\"] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def dataset_load(appliances, train, test=None):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_train_timestamp = []\n",
    "    x_train_mean = []\n",
    "    x_train_std = []\n",
    "    x_train_max_min = []\n",
    "    x_train_main = []\n",
    "    x_train_main_dif = []\n",
    "    x_train_max = []\n",
    "    x_train_min = []\n",
    "    x_train_kurtosis = []\n",
    "    x_train_skew = []\n",
    "    n = 99\n",
    "    m = 9\n",
    "    units_to_pad = n // 2\n",
    "    units_to_pad1 = m//2\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    scaler_mean = StandardScaler()\n",
    "    scaler_std = StandardScaler()\n",
    "    scaler_max_min = StandardScaler()\n",
    "    scaler_main = StandardScaler()\n",
    "    scaler_diff = StandardScaler()\n",
    "    scaler_max = StandardScaler()\n",
    "    scaler_min = StandardScaler()\n",
    "    scaler_kurtosis = StandardScaler()\n",
    "    scaler_skew = StandardScaler()\n",
    "    # train\n",
    "    for key, values in train.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[app]])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            # x = df[\"main\"].values\n",
    "\n",
    "            # x_diff = x\n",
    "            # for i in range(1,len(x)):\n",
    "            #     x_diff[i] = x_diff[i] - x_diff[i-1]\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                print(app)\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "            # \n",
    "            # y = df[appliances[0]].values\n",
    "        x_train_main.extend(x)\n",
    "        timestamp_train = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x2 = x\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        # print(0)\n",
    "        x_train_mean.extend(jnp.mean(x, axis=1))\n",
    "        # print(0)\n",
    "        x_train_std.extend(jnp.std(x, axis=1))\n",
    "        x1 = jnp.pad(x2, (units_to_pad1, units_to_pad1),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x1 = jnp.array([x1[i: i + m] for i in range(len(x1) - m + 1)])\n",
    "        x_train_max_min.extend(jnp.max(x, axis=1)-jnp.min(x,axis=1))\n",
    "        x_train_max.extend(jnp.max(x1, axis = 1))\n",
    "        x_train_min.extend(jnp.min(x1, axis = 1))\n",
    "        x_train_kurtosis.extend(kurtosis(x,axis=1))\n",
    "        x_train_skew.extend(skew(x,axis = 1))\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "        # x_train_diff.extend(x_diff)\n",
    "\n",
    "    # for i in range(1,len(x_train)):\n",
    "    #     x_train[i] = x_train[i] - x_train[i-1]\n",
    "    x_train = jnp.array(x_train)\n",
    "    y_train = jnp.array(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1,1)\n",
    "    x_train_main = jnp.array(x_train_main).reshape(-1,1)\n",
    "    x_train_mean = jnp.array(x_train_mean).reshape(-1,1)\n",
    "    x_train_std =  jnp.array(x_train_std).reshape(-1,1)\n",
    "    x_train_max_min =  jnp.array(x_train_max_min).reshape(-1,1)\n",
    "    x_train_max =  jnp.array(x_train_max).reshape(-1,1)\n",
    "    x_train_skew = jnp.array(x_train_skew).reshape(-1,1)\n",
    "    x_train_min =  jnp.array(x_train_min).reshape(-1,1)\n",
    "    x_train_kurtosis = jnp.array(x_train_kurtosis).reshape(-1,1)\n",
    "    # x_train_diff =  jnp.array(x_train_diff).reshape(-1,1)\n",
    "   \n",
    "\n",
    "\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "    x_train_main = scaler_main.fit_transform(x_train_main)\n",
    "    x_train_mean = scaler_mean.fit_transform(x_train_mean)\n",
    "    x_train_std = scaler_std.fit_transform(x_train_std)\n",
    "    x_train_max_min = scaler_max_min.fit_transform(x_train_max_min)\n",
    "    x_train_max = scaler_max.fit_transform(x_train_max)\n",
    "    x_train_min = scaler_min.fit_transform(x_train_min)\n",
    "    x_train_kurtosis = scaler_kurtosis.fit_transform(x_train_kurtosis)\n",
    "    x_train_skew = scaler_skew.fit_transform(x_train_skew)\n",
    "    # x_train_diff = scaler_diff.fit_transform(x_train_diff)\n",
    "\n",
    "    print(0)\n",
    "    # test\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    x_test_mean = []\n",
    "    x_test_std = []\n",
    "    x_test_max_min = []\n",
    "    x_test_timestamp_true =[]\n",
    "    x_test_main = []\n",
    "    x_test_max = []\n",
    "    x_test_min = []\n",
    "    x_test_kurtosis = []\n",
    "    x_test_skew = []\n",
    "\n",
    "    for key, values in test.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[app]])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            # x = df[\"main\"].values \n",
    "\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                print(app)\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "            # x = x - 100\n",
    "            # x_diff = x\n",
    "            # for i in range(1,len(x)):\n",
    "            #     x_diff[i] = x_diff[i] - x_diff[i-1]\n",
    "            # \n",
    "            # y = df[appliances[0]].values\n",
    "        x_test_main.extend(x)\n",
    "        timestamp_true = df[\"Timestamp\"].values\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x2 = x\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "\n",
    "        x_test_mean.extend(jnp.mean(x, axis=1))\n",
    "        x_test_std.extend(jnp.std(x, axis=1))\n",
    "        x1 = jnp.pad(x2, (units_to_pad1, units_to_pad1),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x1 = jnp.array([x1[i: i + m] for i in range(len(x1) - m + 1)])\n",
    "        x_test_max_min.extend(jnp.max(x, axis=1)-jnp.min(x, axis=1))\n",
    "        x_test_max.extend(jnp.max(x1, axis=1))\n",
    "        x_test_min.extend(jnp.min(x1, axis=1))\n",
    "        x_test.extend(x)\n",
    "        x_test_kurtosis.extend(kurtosis(x,axis=1))\n",
    "        x_test_skew.extend(skew(x,axis=1))\n",
    "        y_test.extend(y)\n",
    "        x_test_timestamp_true.extend(timestamp_true)\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "        # x_test_diff.extend(x_diff)\n",
    "\n",
    "    # for i in range(1,len(x_test)):\n",
    "    #     x_test[i] = x_test[i] - x_test[i-1]\n",
    "    x_test = jnp.array(x_test)\n",
    "    y_test = jnp.array(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1,1)\n",
    "    x_test_main = jnp.array(x_test_main).reshape(-1,1)\n",
    "    x_test_mean = jnp.array(x_test_mean).reshape(-1,1)\n",
    "    x_test_std =  jnp.array(x_test_std).reshape(-1,1)\n",
    "    x_test_max_min =  jnp.array(x_test_max_min).reshape(-1,1)\n",
    "    x_test_max = jnp.array(x_test_max).reshape(-1,1)\n",
    "    x_test_min = jnp.array(x_test_min).reshape(-1,1)\n",
    "    x_test_kurtosis = jnp.array(x_test_kurtosis).reshape(-1,1)\n",
    "    x_test_skew = jnp.array(x_test_skew).reshape(-1,1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "    x_test_mean = scaler_mean.transform(x_test_mean)\n",
    "    x_test_std = scaler_std.transform(x_test_std)\n",
    "    x_test_max_min = scaler_max_min.transform(x_test_max_min)\n",
    "    x_test_main = scaler_main.transform(x_test_main)\n",
    "    x_test_max =  scaler_max.transform(x_test_max)\n",
    "    x_test_min = scaler_min.transform(x_test_min)\n",
    "    x_test_kurtosis = scaler_kurtosis.transform(x_test_kurtosis)\n",
    "    x_test_skew = scaler_skew.transform(x_test_skew)\n",
    "    # x_test_diff = scaler_diff.transform(x_test_diff)\n",
    "#     y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    # x_train_features = x_train_main_dif\n",
    "    # x_test_features = x_test_main_dif\n",
    "\n",
    "    x_train = jnp.array(x_train[:,1:]).reshape(x_train.shape[0], n-1)\n",
    "    y_train = jnp.array(y_train)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(x_train_timestamp.shape[0], 1)\n",
    "    x_test = jnp.array(x_test[:,1:]).reshape(x_test.shape[0], n-1)\n",
    "    y_test = jnp.array(y_test)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(x_test_timestamp.shape[0], 1).to(torch.float64)\n",
    "\n",
    "    #x_train_std, \n",
    "    #, x_test_std, x_test_max_min,\n",
    "    num_features_selected = 6\n",
    "    x_train_features = jnp.concatenate((x_train_main, x_train_mean,  x_train_max_min,x_train_max, x_train_min, x_train_kurtosis), axis=1).reshape(x_train.shape[0], num_features_selected)\n",
    "    x_test_features = jnp.concatenate((x_test_main, x_test_mean,  x_test_max_min, x_test_max, x_test_min,  x_test_kurtosis), axis=1).reshape(x_test.shape[0], num_features_selected)\n",
    "\n",
    "    scalers = np.array([scaler_x, scaler_y, scaler_time, scaler_main, scaler_mean, scaler_std, scaler_max_min, scaler_max, scaler_min, scaler_kurtosis, scaler_skew])\n",
    "    return x_train, y_train, x_test, y_test, x_train_features, x_test_features, x_train_timestamp, x_test_timestamp, scalers, x_test_main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, x_train_features, x_test_features, x_train_timstamp, x_test_timestamp, scalers, x_test_main = dataset_load(appliances, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_diff = x_train_features[:,0]\n",
    "diff = np.array(x_train_diff)\n",
    "for i in range(1, len(x_train_diff)):\n",
    "\t\tvalue = x_train_diff[i] - x_train_diff[i - 1]\n",
    "\t\tdiff[i] = value\n",
    "\n",
    "x_test_diff = x_test_features[:,0]\n",
    "diff = np.array(x_test_diff)\n",
    "for i in range(1, len(x_test_diff)):\n",
    "\t\tvalue = x_test_diff[i] - x_test_diff[i - 1]\n",
    "\t\tdiff[i] = value\n",
    "\n",
    "# x_train_zeros = np.zeros(x_train_diff.shape)\n",
    "# x_test_zeros = np.zeros(x_test_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = jnp.concatenate((x_train_features, jnp.array(np.array(x_train_diff.reshape(-1,1)))), axis=1)\n",
    "x_test_features = jnp.concatenate((x_test_features, jnp.array(np.array(x_test_diff.reshape(-1,1)))), axis=1)\n",
    "\n",
    "#, jnp.array(x_test_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24918, 7), (15656, 7))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape, x_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "indexes = 53456\n",
    "n = x_train_features.shape[1]\n",
    "x = torch.tensor(np.array(x_train_features)).to(torch.float64)\n",
    "y = torch.tensor(np.array(y_train)).reshape(-1,).to(torch.float64)\n",
    "xt = torch.tensor(np.array(x_test_features)).to(torch.float64)\n",
    "yt = torch.tensor(np.array(y_test)).reshape(-1,).to(torch.float64)\n",
    "\n",
    "if x.shape[0]>indexes:\n",
    "  x = x[:indexes]\n",
    "  y = y[:indexes]\n",
    "\n",
    "# xt = xt[:16000]\n",
    "# yt = yt[:16000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15656, 7, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class seq2point(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(seq2point, self).__init__()\n",
    "        self.c1 = nn.Conv1d(7,7, kernel_size=1)\n",
    "        self.r1 = nn.ReLU()\n",
    "        self.c2 = nn.Conv1d(7,3,kernel_size=1)\n",
    "        self.r2 = nn.ReLU()        \n",
    "        self.c3 = nn.Conv1d(3,1, kernel_size=1)\n",
    "        self.r3 = nn.ReLU()\n",
    "        # c4 = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        # X = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        # X = X.reshape((X.shape[0], -1))\n",
    "        # X = nn.Dense(1024)(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        self.d = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self,X):\n",
    "        print(f\"foawrd is running\")\n",
    "        X = self.c1(X)\n",
    "        print(f\"shape of X1 -> {X.shape}\")\n",
    "\n",
    "        X = self.r1(X)\n",
    "        print(f\"shape of X2 -> {X.shape}\")\n",
    "\n",
    "        X = self.c2(X)\n",
    "        print(f\"shape of X3 -> {X.shape}\")\n",
    "\n",
    "        X = self.r2(X) \n",
    "        print(f\"shape of X4 -> {X.shape}\")\n",
    "\n",
    "        X = self.c3(X)\n",
    "        print(f\"shape of X5 -> {X.shape}\")\n",
    "\n",
    "        X = self.r3(X)\n",
    "        print(f\"shape of X6 -> {X.shape}\")\n",
    "        # X = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        # X = nn.Conv(50, kernel_size=(5,))(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        # X = X.reshape((X.shape[0], -1))\n",
    "        # X = nn.Dense(1024)(X)\n",
    "        # X = nn.ReLU(X)\n",
    "        # X = nn.Dropout(rate=0.2, deterministic=deterministic)(X)\n",
    "        X = self.d(X)\n",
    "        print(f\"final shape -> {X.shape}\")\n",
    "        return X\n",
    "\n",
    "feature_extractor1 = seq2point()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            print(\"GPRegressionModel initialized\")\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            print(\"mean instantiated\")\n",
    "            self.base_covar_module = ( gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5,ard_num_dims=n)))\n",
    "            print(\"covariance insitantiated\")\n",
    "            #+ gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=(49)))*gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel(ard_num_dims=1, active_dims=(49)))\n",
    "            # self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            #     gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "            #     num_dims=2, grid_size=100\n",
    "            # )\n",
    "            self.likelihood = likelihood\n",
    "            # self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[np.arange(0, train_x.shape[0], 95)], likelihood=likelihood)\n",
    "            # print(self.covar_module)\n",
    "            self.feature_extractor1 = feature_extractor1\n",
    "            # self.feature_extractor2 = feature_extractor2\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            # self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x1):\n",
    "            print(\"GPRegression forward is starting\")\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x = self.feature_extractor1(x1)\n",
    "            print(\"Testing if feature extractor works\")\n",
    "            print(projected_x.shape)\n",
    "            # print(projected_x.shape, hn.shape,cn.shape)\n",
    "            # projected_x = self.feature_extractor2(projected_x)\n",
    "            # print(projected_x.shape, hn.shape,cn.shape)\n",
    "            # projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "            # project_x = projected_x\n",
    "            # plt.figure()\n",
    "            # plt.plot(projected_x.detach().cpu()[:,49],x1.detach().cpu()[:,49])\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "            print(projected_x.dtype)\n",
    "            projected_x = projected_x.squeeze(-1)\n",
    "            print(f\"new x shape -> {projected_x.shape}\")\n",
    "            self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=projected_x[np.arange(0, projected_x.shape[0], 95)], likelihood=self.likelihood)\n",
    "            print(\"covariance matrix sampled\")\n",
    "            mean_x = self.mean_module(projected_x) #projected_\n",
    "            print(\"mean calculated\")\n",
    "           \n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            print(\"covariance matrix generated\")\n",
    "            \n",
    "            ret_val =  gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "            print(f\"distribution formed\")\n",
    "            print(ret_val)\n",
    "            return ret_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPRegressionModel initialized\n",
      "mean instantiated\n",
      "covariance insitantiated\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(x, y, likelihood)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "#     likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foawrd is running\n",
      "shape of X1 -> torch.Size([24918, 7, 1])\n",
      "shape of X2 -> torch.Size([24918, 7, 1])\n",
      "shape of X3 -> torch.Size([24918, 3, 1])\n",
      "shape of X4 -> torch.Size([24918, 3, 1])\n",
      "shape of X5 -> torch.Size([24918, 1, 1])\n",
      "shape of X6 -> torch.Size([24918, 1, 1])\n",
      "final shape -> torch.Size([24918, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2248320/1843471022.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24918, 1, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(24918,7,1)\n",
    "x.shape\n",
    "x = torch.tensor(x)\n",
    "\n",
    "# x_try = torch.randn(64,7,100)\n",
    "trial = feature_extractor1(x)\n",
    "trial.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1bff9345794e67a1cec5e588879630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPRegression forward is starting\n",
      "foawrd is running\n",
      "shape of X1 -> torch.Size([24918, 7, 1])\n",
      "shape of X2 -> torch.Size([24918, 7, 1])\n",
      "shape of X3 -> torch.Size([24918, 3, 1])\n",
      "shape of X4 -> torch.Size([24918, 3, 1])\n",
      "shape of X5 -> torch.Size([24918, 1, 1])\n",
      "shape of X6 -> torch.Size([24918, 1, 1])\n",
      "final shape -> torch.Size([24918, 1, 1])\n",
      "Testing if feature extractor works\n",
      "torch.Size([24918, 1, 1])\n",
      "torch.float32\n",
      "new x shape -> torch.Size([24918, 1])\n",
      "covariance matrix sampled\n",
      "mean calculated\n",
      "covariance matrix generated\n",
      "distribution formed\n",
      "MultivariateNormal(loc: torch.Size([24918]))\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desai.aadesh/.local/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/home/desai.aadesh/.local/lib/python3.8/site-packages/gpytorch/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPRegression forward is starting\n",
      "foawrd is running\n",
      "shape of X1 -> torch.Size([24918, 7, 1])\n",
      "shape of X2 -> torch.Size([24918, 7, 1])\n",
      "shape of X3 -> torch.Size([24918, 3, 1])\n",
      "shape of X4 -> torch.Size([24918, 3, 1])\n",
      "shape of X5 -> torch.Size([24918, 1, 1])\n",
      "shape of X6 -> torch.Size([24918, 1, 1])\n",
      "final shape -> torch.Size([24918, 1, 1])\n",
      "Testing if feature extractor works\n",
      "torch.Size([24918, 1, 1])\n",
      "torch.float32\n",
      "new x shape -> torch.Size([24918, 1])\n",
      "covariance matrix sampled\n",
      "mean calculated\n",
      "covariance matrix generated\n",
      "distribution formed\n",
      "MultivariateNormal(loc: torch.Size([24918]))\n",
      "0\n",
      "CPU times: user 49.1 s, sys: 525 ms, total: 49.7 s\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "training_iterations = 2\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "loss_arr =[]\n",
    "def train():\n",
    "    iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output= model(x.to(torch.float32))\n",
    "        # print(output.loc.shape)\n",
    "        # Calc loss and backprop derivatives\n",
    "        print(0)\n",
    "        loss = -mll(output, y.to(torch.float32))\n",
    "        # loss_arr.append(loss.cpu())\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    " \n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deep_kernel.pt\"\n",
    "torch.save(model.state_dict(), os.path.join(\n",
    "  \"Neurips/models\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xt = xt.squeeze(-1)\n",
    "# xt.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing \n",
    "# try_x = torch.randn(15656, 7, 1)\n",
    "# new_model = seq2point()\n",
    "# print(new_model(try_x).shape)\n",
    "# preds = model(try_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15656, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    xt_new = xt.reshape(-1,7,1).to(torch.float32)\n",
    "    # print(xt_new.shape)\n",
    "    preds= model(xt_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.array(preds.mean.cpu())\n",
    "y_mean = scalers[1].inverse_transform(y_mean.reshape(-1,1)).squeeze()\n",
    "y_mean2 = np.clip(y_mean,0,y_mean.max(),out=y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(torch.tensor(y_mean2) - yt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_plot = scalers[2].inverse_transform(x_test_timestamp.reshape(-1,1))\n",
    "x1 = x_test_features[:,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1000 #x2.shape[0]\n",
    "start = 4000\n",
    "plt.figure(figsize=(10,6))\n",
    "# plt.plot(time_plot[start:start+idx], scalers[3].inverse_transform(x1)[start:start+idx], label = \"Main\")\n",
    "# plt.plot(x_test_timestamp[:idx], scalers[-1].inverse_transform(x2)[:idx], label = \"Main Ref Dif\")\n",
    "plt.plot(time_plot[start:start+idx], yt.cpu()[start:start+idx], label = \"Refri\")\n",
    "plt.plot(time_plot[start:start+idx], y_mean[start:start+idx], label = \"Predicted\")\n",
    "plt.legend(bbox_to_anchor=(1, 1),fontsize=20)\n",
    "plt.xlabel(\"TimeStamp test Scaled\")\n",
    "plt.ylabel(\"Fridge  test Power\")\n",
    "plt.title(\"Building 6\")\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
