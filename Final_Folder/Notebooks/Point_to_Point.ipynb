{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, PeriodicKernel, MaternKernel, CosineKernel\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor, ExactGPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from utilities.fits import fit\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.constraints import GreaterThan\n",
    "# from skgpytorch.metrics import mean_squared_error, negative_log_predictive_density\n",
    "from gpytorch.metrics import mean_standardized_log_loss, negative_log_predictive_density,  mean_squared_error\n",
    "import time\n",
    "import numpy as np\n",
    "# from utilities import plot,fits,gmm,errors,predict,preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latexifying Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "\n",
    "os.environ[\"LATEXIFY\"] = \"1\"\n",
    "os.environ[\"FIG_DIR\"] = \"Neurips/FinalPlots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load(appliances, train, test=None, bias = False):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    train_time = []\n",
    "    x_train_timestamp = []\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    app = 0\n",
    "\n",
    "    ### train\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    for key, values in train.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[app]])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "            \n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "\n",
    "        timetrain = df[\"Timestamp\"]\n",
    "        timestamp_train = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "\n",
    "        x_train.extend(torch.tensor(x))\n",
    "        y_train.extend(torch.tensor(y))\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "        train_time.extend(timetrain)\n",
    "\n",
    "    x_train = torch.tensor(x_train).reshape(-1, 1)\n",
    "    y_train = torch.tensor(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1,1)\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "\n",
    "\n",
    "    ## test\n",
    "    x_test = []\n",
    "    test_time = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    app = 0\n",
    "    for key, values in test.items():\n",
    "        for app in range(len(appliances)):\n",
    "            df = pd.read_csv(\n",
    "                f\"Data/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[app]])\n",
    "            df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "            startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "            endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "            if startDate > endDate:\n",
    "                raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "            df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "            df.dropna(inplace=True)\n",
    "            if app == 0:\n",
    "                x = df[appliances[app]].values\n",
    "            else:\n",
    "                x += df[appliances[app]].values\n",
    "            if appliances[app] == \"Refrigerator\":\n",
    "                y = df[appliances[app]].values\n",
    "           \n",
    "        timetest = df[\"Timestamp\"]\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        \n",
    "        if bias==True:\n",
    "            x = x + 100*np.ones(x.shape[0])\n",
    "        x_test.extend(torch.tensor(x))\n",
    "        y_test.extend(torch.tensor(y))\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "        test_time.extend(timetest)\n",
    "\n",
    "    x_test = torch.tensor(x_test).reshape(-1, 1)\n",
    "    y_test = torch.tensor(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1,1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "\n",
    "    x_train = torch.tensor(x_train).reshape(x_train.shape[0], 1).to(torch.float32)\n",
    "    y_train = torch.tensor(y_train).reshape(-1,).to(torch.float32)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(x_train_timestamp.shape[0], 1).to(torch.float32)\n",
    "    x_test = torch.tensor(x_test).reshape(x_test.shape[0], 1).to(torch.float32)\n",
    "    y_test = torch.tensor(y_test).reshape(-1,).to(torch.float32)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(x_test_timestamp.shape[0], 1).to(torch.float32)\n",
    "\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_train_timestamp, x_test_timestamp, scaler_x, scaler_y, scaler_time, test_time, train_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={\n",
    "    \n",
    "    1:{\n",
    "                    'start_time': \"2011-04-28\" ,\n",
    "                    'end_time': \"2011-05-15\"\n",
    "                },\n",
    "        3: {\n",
    "                'start_time': \"2011-04-19\" ,\n",
    "                'end_time': \"2011-05-22\"\n",
    "            }\n",
    "                }\n",
    "test = { 2: {\n",
    "                    'start_time': \"2011-04-21\" ,\n",
    "                    'end_time': \"2011-05-21\"\n",
    "                },\n",
    "                }\n",
    "\n",
    "appliances = [\"Microwave\", \"Refrigerator\",  \"Dish Washer\"] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_train_timestamp, x_test_timestamp, scaler_x, scaler_y, scaler_time, test_time, train_time = dataset_load(appliances, train, test, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_train_timestamp.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full = x_train\n",
    "y_train = y_train\n",
    "x_test_full = x_test\n",
    "x_train_full.shape, x_test_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_model(train, test):\n",
    "        if (train):\n",
    "                kernel1 = ScaleKernel(MaternKernel(nu=2.5, ard_num_dims=1, active_dims=(0)))\n",
    "                kernel = kernel1\n",
    "                inducing_points =  x_train_full[np.arange(0,x_train_full.shape[0],20)]\n",
    "                model = SGPRegressor(x_train_full.to(\"cuda\"), y_train.to(\"cuda\"), kernel,\n",
    "                                inducing_points).to(\"cuda\")\n",
    "\n",
    "                loss = model.fit(lr=1e-3, n_epochs=3000,verbose=1,thetas=None,\n",
    "                        random_state=0)\n",
    "                plt.plot(np.asarray(loss[0]))  \n",
    "\n",
    "                ## Save model\n",
    "                model_name = \"Point_to_point_main_power.pt\"\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                \"Neurips/models\", model_name))\n",
    "        if (test):\n",
    "                kernel1 = ScaleKernel(MaternKernel(nu=2.5))\n",
    "                kernel = kernel1\n",
    "                inducing_points =  x_train_full[np.arange(0,x_train_full.shape[0],20)] \n",
    "                model = SGPRegressor(x_train_full.to(\"cuda\"), y_train.to(\"cuda\"), kernel,\n",
    "                                inducing_points).to(\"cuda\")\n",
    "                model_name = \"Point_to_point_main_power.pt\"\n",
    "                model.load_state_dict(torch.load(\n",
    "                os.path.join(\"./models\", model_name)))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GP_model(train=False, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "latexify(width_scale_factor=2.5, fig_height=1.75)\n",
    "sns.kdeplot(data = {\"Train Appliance\": scaler_y.inverse_transform(y_train.reshape(-1,1).cpu()).squeeze(),\"Test Appliance\": (y_test.cpu()).squeeze() })\n",
    "sns.despine()\n",
    "# savefig(\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin_max = 3000\n",
    "x_lin = np.linspace(0,x_lin_max,15656) #+ 100*np.ones(np.array(x_test).shape[0])\n",
    "x_time = np.linspace(scaler_time.inverse_transform(x_test_timestamp).min(), scaler_time.inverse_transform(x_test_timestamp).max(), 15656)\n",
    "x_lin_scale = scaler_x.transform(x_lin.reshape(-1,1)).flatten() \n",
    "x_new =torch.tensor(x_lin_scale).reshape(-1,1).to(torch.float32)\n",
    "\n",
    "x_new.shape, x_new.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = model.predict(x_new.to(\"cuda\"))\n",
    "y_mean = pred_dist.loc \n",
    "y_mean = scaler_y.inverse_transform(y_mean.cpu().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "latexify(width_scale_factor=2, fig_height=1.5)\n",
    "start = 500\n",
    "idx = 4000\n",
    "plt.plot(x_lin, y_mean, 'k', label=\" Predicted Mean\", alpha=0.7)\n",
    "plt.scatter( scaler_x.inverse_transform(x_train_full[:,0].reshape(-1,1)), scaler_y.inverse_transform(y_train.reshape(-1,1)), s = 4, label= \"Appliance Power\")\n",
    "plt.xlim(00,1500)\n",
    "sns.despine()\n",
    "# plt.title(\"Train Mains Vs Train Applaince along with Predicted Means\")\n",
    "\n",
    "plt.xlabel(\"Train Mains\")\n",
    "plt.ylabel(\"Train Appliance Power\")\n",
    "# plt.show()\n",
    "plt.axvline(x=145,color='olive', linestyle='dotted',label = \"Mains = ~150\")\n",
    "plt.axvline(x=188,color='red', linestyle='dotted',label = \"Mains = ~188\")\n",
    "\n",
    "plt.axvline(x=490,color='magenta', linestyle='dotted')\n",
    "# # plt.axvline(x=430,color='red', linestyle='dotted',label = \"Mains = ~188\")\n",
    "plt.axvline(x=1250,color='brown', linestyle='dotted')\n",
    "\n",
    "plt.legend( frameon = False, fontsize = 6, bbox_to_anchor=(0.35, 0.55))\n",
    "# savefig(\"Main_vs_app_mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = test_time\n",
    "\n",
    "x_ticks_labels = pd.to_datetime(values)\n",
    "x_ticks_labels\n",
    "time_ = [(i.split('-04:00')[0].strip()) for i in test_time[:]]\n",
    "\n",
    "date = [(i.split(' ')[0].strip()) for i in time_[:]]\n",
    "mins_data = [(i.split(' ')[1].strip()) for i in time_[:]]\n",
    "secs = [(i.split(':00')[1].strip()) for i in time_[:]]\n",
    "def date_con(input_string:str):\n",
    "  year,month,day = input_string.split(\"-\")\n",
    "  ret_month = ''\n",
    "  if(int(month)==4):\n",
    "    ret_month = 'April'\n",
    "  elif(int(month)==5):\n",
    "    ret_month  = 'May'\n",
    "  \n",
    "  ret_string  = f\"{day} {ret_month}\"\n",
    "  return ret_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 100\n",
    "fig, ax = plt.subplots(1,1) \n",
    "start = 13100\n",
    "time_plot = scaler_time.inverse_transform(x_test_timestamp.cpu().reshape(-1,1))\n",
    "latexify(width_scale_factor=3, fig_height=1.75)\n",
    "ax.scatter(time_plot[start : start + idx], scaler_x.inverse_transform(x_train)[start : start + idx], label = \"Train Main\", s = 6)\n",
    "mins = mins_data[start : start + idx]\n",
    "dates = date[start : start + idx]\n",
    "ax.set_ylabel(\"Train Mains Power\" )\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False,\n",
    ")\n",
    "ax.set_xlabel(\"Time\" + \"\\n\" + date_con(dates[0]) +\" (\" + mins[0][:-3]+ \") \" + \"to  \" + date_con(dates[-1])+ \" (\" + mins[-1][:-3]+ \")\")\n",
    "sns.despine()\n",
    "# plt.show()\n",
    "# savefig(\"Train_Scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dist = model.predict((x_test_full).to(\"cuda\"))\n",
    "y_mean = pred_dist.loc \n",
    "y_mean = scaler_y.inverse_transform(y_mean.reshape(-1,1).cpu()).squeeze()\n",
    "\n",
    "print(y_test.shape, y_mean.shape)\n",
    "y_mean = np.clip(y_mean,0,y_mean.max(),out=y_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = torch.abs(torch.tensor(y_mean) - y_test).mean(dim=-1)\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "var_pred = pred_dist.variance\n",
    "var_pred = scaler_y.inverse_transform(var_pred.reshape(-1,1).detach().cpu()).squeeze()\n",
    "# msll = mean_standardized_log_loss(torch.ensor(y_mean), y_test)\n",
    "f_var= torch.tensor(var_pred)\n",
    "f_mean = torch.tensor(y_mean)\n",
    "msll = 0.5 * (torch.log(2 * pi * f_var) + torch.square(y_test - f_mean) / (2 * f_var)).mean(dim=-1)\n",
    "msll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# def log_prob(y_pred, value):\n",
    "#         r\"\"\"\n",
    "#         See :py:meth:`torch.distributions.Distribution.log_prob\n",
    "#         <torch.distributions.distribution.Distribution.log_prob>`.\n",
    "#         \"\"\"\n",
    "\n",
    "\n",
    "#         mean, covar = y_pred.loc.cpu(), y_pred.variance.cpu()\n",
    "#         diff = value - mean\n",
    "\n",
    "#         # Repeat the covar to match the batch shape of diff\n",
    "#         # if diff.shape[:-1] != covar.batch_shape:\n",
    "#         #     if len(diff.shape[:-1]) < len(covar.batch_shape):\n",
    "#         #         diff = diff.expand(covar.shape[:-1])\n",
    "#         #     else:\n",
    "#         #         padded_batch_shape = (*(1 for _ in range(diff.dim() + 1 - covar.dim())), *covar.batch_shape)\n",
    "#         #         covar = covar.repeat(\n",
    "#         #             *(diff_size // covar_size for diff_size, covar_size in zip(diff.shape[:-1], padded_batch_shape)),\n",
    "#         #             1,\n",
    "#         #             1,\n",
    "#         #         )\n",
    "\n",
    "#         # Get log determininant and first part of quadratic form\n",
    "#         covar = covar.evaluate_kernel()\n",
    "#         inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n",
    "\n",
    "#         res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])\n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_pred = pred_dist.variance\n",
    "# var_pred = scaler_y.inverse_transform(var_pred.reshape(-1,1).detach().cpu()).squeeze()\n",
    "# # msll = mean_standardized_log_loss(torch.ensor(y_mean), y_test)\n",
    "# f_var= torch.tensor(var_pred)\n",
    "# f_mean = torch.tensor(y_mean)\n",
    "# nlpd = -pred_dist.log_prob(y_test) / y_test.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QCE 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 95.0\n",
    "standard_normal = torch.distributions.Normal(loc=0.0, scale=1.0)\n",
    "deviation = standard_normal.icdf(torch.as_tensor(0.5 + 0.5 * (quantile / 100)))\n",
    "std_pred = pred_dist.stddev\n",
    "std_pred = torch.tensor(scaler_y.inverse_transform(std_pred.reshape(-1,1).detach().cpu()).squeeze())\n",
    "lower = torch.tensor(y_mean) - deviation * std_pred \n",
    "upper = torch.tensor(y_mean) + deviation * std_pred \n",
    "n_samples_within_bounds = ((y_test > lower) * (y_test < upper)).sum(-1)\n",
    "fraction = n_samples_within_bounds / y_test.shape[-1]\n",
    "qce = torch.abs(fraction - quantile / 100)\n",
    "qce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Var plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 500\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5)) \n",
    "start = 4000\n",
    "var_pred = pred_dist.variance\n",
    "var_pred = scaler_y.inverse_transform(var_pred.reshape(-1,1).detach().cpu()).squeeze()\n",
    "time_plot = scaler_time.inverse_transform(x_test_timestamp.cpu().reshape(-1,1))\n",
    "# latexify(width_scale_factor=2, fig_height=1.75)\n",
    "ax.plot(time_plot[start : start + idx], scaler_x.inverse_transform(x_test_full.cpu().reshape(-1,1))[start : start + idx], label = \"Main Power\")\n",
    "ax.plot(time_plot[start : start + idx], y_test.cpu()[start : start + idx], label = \"Ground Truth\")\n",
    "ax.plot(time_plot[start : start + idx], y_mean[start : start + idx], label = \"Prediction\")\n",
    "ax.fill_between(time_plot[start : start + idx].flatten(), y_mean[start : start + idx].flatten() - 1.96*np.sqrt(var_pred[start : start + idx]).flatten(),  y_mean[start : start + idx].flatten() + 1.96*np.sqrt(var_pred[start : start + idx]).flatten(),  color='lightblue', alpha=1.0, label = \"Confidence Interval 95%)\")\n",
    "ax.errorbar(time_plot[start : start + idx].flatten(), y_mean[start : start + idx].flatten(), yerr=np.sqrt(var_pred[start : start + idx]).flatten(), fmt='k*', label = \"error_bar\")\n",
    "plt.legend(frameon=False)\n",
    "ax.set_ylabel(\"Power\" )\n",
    "sns.despine()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1 Plots (4800:200, 13000:300, 4000:500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 200 #x2.shape[0]\n",
    "start = 4800\n",
    "\n",
    "plt.figure()\n",
    "latexify(width_scale_factor=2, fig_height=1.75)\n",
    "df = pd.read_csv(\"./time_stamp.csv\", index_col=0)\n",
    "df.index = df[\"0\"]\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df.index.name = \"Time\"\n",
    "df = df.drop(columns=[\"0\"])\n",
    "df[\"Main Power\"] = scaler_x.inverse_transform(x_test_full.cpu().reshape(-1,1) )\n",
    "df[\"Ground Truth\"] = y_test.cpu()\n",
    "df[\"Prediction\"] = y_mean\n",
    "df[start:start+idx].plot(rot=90)\n",
    "sns.despine()\n",
    "# df.plot.( rotation=90)\n",
    "plt.legend(frameon=False, bbox_to_anchor=(0.5, 0.5))\n",
    "# df.head()\n",
    "# plt.ylim(0,350)\n",
    "plt.ylabel(\"Power\")\n",
    "# savefig(\"Point_to_point_plt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in model.named_parameters():\n",
    "  print(name, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
