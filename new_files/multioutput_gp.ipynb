{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, PeriodicKernel, MaternKernel, CosineKernel\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utilities.fits import fit\n",
    "# from utilities import plot\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from skgpytorch.metrics import mean_squared_error, negative_log_predictive_density\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# from datasets.dataset_load import dataset_loader\n",
    "from utilities import plot,fits,gmm,errors,predict,preprocess\n",
    "\n",
    "# device = \"cpu\"\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def dataset_load(appliances, train, test=None):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_train_timestamp = []\n",
    "    n = 9\n",
    "    units_to_pad = n // 2\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    # train\n",
    "    for key, values in train.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp_train = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "\n",
    "\n",
    "    x_train = jnp.array(x_train)\n",
    "    y_train = jnp.array(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1,1)\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "\n",
    "\n",
    "    # test\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    for key, values in test.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_test.extend(x)\n",
    "        y_test.extend(y)\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "\n",
    "    x_test = jnp.array(x_test)\n",
    "    y_test = jnp.array(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1,1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "#     y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    x_train = jnp.array(x_train).reshape(x_train.shape[0], n, 1)\n",
    "    y_train = jnp.array(y_train)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(x_train_timestamp.shape[0], 1)\n",
    "    x_test = jnp.array(x_test).reshape(x_test.shape[0], n, 1)\n",
    "    y_test = jnp.array(y_test)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(x_test_timestamp.shape[0], 1).to(torch.float64)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_train_timestamp, x_test_timestamp, scaler_x, scaler_y, scaler_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={\n",
    "    1: {\n",
    "          'start_time': \"2011-04-28\" ,\n",
    "          'end_time': \"2011-05-15\"\n",
    "        },\n",
    "    3: {\n",
    "          'start_time': \"2011-04-19\" ,\n",
    "          'end_time': \"2011-05-22\"\n",
    "    }\n",
    "}\n",
    "test = {\n",
    "     2: {\n",
    "          'start_time': \"2011-04-21\" ,\n",
    "          'end_time': \"2011-05-21\"\n",
    "    }\n",
    "}\n",
    "appliances = [\"Dish Washer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, y_train1, x_test1, y_test1, x_train_timstamp1, x_test_timestamp1, scaler_x1, scaler_y1, scaler_time1 = dataset_load(appliances, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = torch.tensor(np.array(x_train1).reshape(-1,9)).to(torch.float64)\n",
    "y_train1 = torch.tensor(np.array(y_train1)).reshape(-1,).to(torch.float64)\n",
    "x_test1 = torch.tensor(np.array(x_test1).reshape(-1,9)).to(torch.float64)\n",
    "y_test1 = torch.tensor(np.array(y_test1)).reshape(-1,).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={\n",
    "    1: {\n",
    "          'start_time': \"2011-04-28\" ,\n",
    "          'end_time': \"2011-05-15\"\n",
    "        },\n",
    "    3: {\n",
    "          'start_time': \"2011-04-19\" ,\n",
    "          'end_time': \"2011-05-22\"\n",
    "    }\n",
    "}\n",
    "test = {\n",
    "     2: {\n",
    "          'start_time': \"2011-04-21\" ,\n",
    "          'end_time': \"2011-05-21\"\n",
    "    }\n",
    "}\n",
    "appliances = [\"Microwave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, y_train2, x_test2, y_test2, x_train_timstamp2, x_test_timestamp2, scaler_x2, scaler_y2, scaler_time2 = dataset_load(appliances, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = torch.tensor(np.array(x_train2).reshape(-1,9)).to(torch.float64)\n",
    "y_train2 = torch.tensor(np.array(y_train2)).reshape(-1,).to(torch.float64)\n",
    "x_test2 = torch.tensor(np.array(x_test2).reshape(-1,9)).to(torch.float64)\n",
    "y_test2 = torch.tensor(np.array(y_test2)).reshape(-1,).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def dataset_load(appliances, train, test=None):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_train_timestamp = []\n",
    "    n = 99\n",
    "    units_to_pad = n // 2\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    # train\n",
    "    for key, values in train.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp_train = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "        x_train_timestamp.extend(torch.tensor(timestamp_train))\n",
    "\n",
    "\n",
    "    x_train = jnp.array(x_train)\n",
    "    y_train = jnp.array(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(-1,1)\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "\n",
    "\n",
    "    # test\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    for key, values in test.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_test.extend(x)\n",
    "        y_test.extend(y)\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "\n",
    "    x_test = jnp.array(x_test)\n",
    "    y_test = jnp.array(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(-1,1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "#     y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    x_train = jnp.array(x_train).reshape(x_train.shape[0], n, 1)\n",
    "    y_train = jnp.array(y_train)\n",
    "    x_train_timestamp = torch.tensor(x_train_timestamp).reshape(x_train_timestamp.shape[0], 1)\n",
    "    x_test = jnp.array(x_test).reshape(x_test.shape[0], n, 1)\n",
    "    y_test = jnp.array(y_test)\n",
    "    x_test_timestamp = torch.tensor(x_test_timestamp).reshape(x_test_timestamp.shape[0], 1).to(torch.float64)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_train_timestamp, x_test_timestamp, scaler_x, scaler_y, scaler_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={1:{\n",
    "                    'start_time': \"2011-04-21\" ,\n",
    "                    'end_time': \"2011-05-21\"\n",
    "                },\n",
    "                3: {\n",
    "                    'start_time': \"2011-04-19\" ,\n",
    "                    'end_time': \"2011-05-22\"\n",
    "                },\n",
    "                2: {\n",
    "                    'start_time': \"2011-04-21\" ,\n",
    "                    'end_time': \"2011-05-21\"\n",
    "                },\n",
    "                5: {\n",
    "                    'start_time': \"2011-04-22\" ,\n",
    "                    'end_time': \"2011-06-01\"\n",
    "                }\n",
    "                }\n",
    "test = {6: {\n",
    "                    'start_time': \"2011-05-25\" ,\n",
    "                    'end_time': \"2011-06-13\"\n",
    "                }}\n",
    "appliances = [\"Refrigerator\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3, y_train3, x_test3, y_test3, x_train_timstamp3, x_test_timestamp3, scaler_x3, scaler_y3, scaler_time3 = dataset_load(appliances, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3 = torch.tensor(np.array(x_train3).reshape(-1,99)).to(torch.float64)\n",
    "y_train3 = torch.tensor(np.array(y_train3)).reshape(-1,).to(torch.float64)\n",
    "x_test3 = torch.tensor(np.array(x_test3).reshape(-1,99)).to(torch.float64)\n",
    "y_test3 = torch.tensor(np.array(y_test3)).reshape(-1,).to(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, ScaleKernel, PeriodicKernel, MaternKernel, CosineKernel\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel4 = ScaleKernel(RBFKernel(ard_num_dims=9))\n",
    "kernel = kernel4 # + kernel1*kernel5\n",
    "# induce_points = 512\n",
    "inducing_points =  x[np.arange(0,x.shape[0],20)] \n",
    "# inducing_points = x[torch.randperm(x.shape[0])[: 1500]]\n",
    "\n",
    "model = SGPRegressor(x.to(\"cuda\"), y.to(\"cuda\"), kernel,\n",
    "                      inducing_points).to(\"cuda\")\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood1 = gpytorch.likelihoods.GaussianLikelihood().to(\"cuda\")\n",
    "model1 = ExactGPModel(x_train1.to(\"cuda\"), y_train1.to(\"cuda\"), likelihood1).to(\"cuda\")\n",
    "\n",
    "likelihood2 = gpytorch.likelihoods.GaussianLikelihood().to(\"cuda\")\n",
    "model2 = ExactGPModel(x_train2.to(\"cuda\"), y_train2.to(\"cuda\"), likelihood2).to(\"cuda\")\n",
    "\n",
    "# likelihood3 = gpytorch.likelihoods.GaussianLikelihood().to(\"cuda\")\n",
    "# model3 = ExactGPModel(x_train3.to(\"cuda\"), y_train3.to(\"cuda\"), likelihood3).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gpytorch.models.IndependentModelList(model1, model2)\n",
    "likelihood = gpytorch.likelihoods.LikelihoodList(model1.likelihood, model2.likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.mlls import SumMarginalLogLikelihood\n",
    "\n",
    "mll = SumMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.63 GiB (GPU 0; 79.17 GiB total capacity; 4.63 GiB already allocated; 2.31 GiB free; 4.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb#ch0000018vscode-remote?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb#ch0000018vscode-remote?line=12'>13</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39mmodel\u001b[39m.\u001b[39mtrain_inputs)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb#ch0000018vscode-remote?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output, model\u001b[39m.\u001b[39;49mtrain_targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb#ch0000018vscode-remote?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/desai.aadesh/NLIM/new_files/multioutput_gp.ipynb#ch0000018vscode-remote?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m - Loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, training_iterations, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/mlls/sum_marginal_log_likelihood.py:33\u001b[0m, in \u001b[0;36mSumMarginalLogLikelihood.forward\u001b[0;34m(self, outputs, targets, *params)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    outputs: (Iterable[MultivariateNormal]) - the outputs of the latent function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m        (e.g. parameters in case of heteroskedastic likelihoods)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(params) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     sum_mll \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(mll(output, target) \u001b[39mfor\u001b[39;49;00m mll, output, target \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlls, outputs, targets))\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     sum_mll \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m     36\u001b[0m         mll(output, target, \u001b[39m*\u001b[39miparams)\n\u001b[1;32m     37\u001b[0m         \u001b[39mfor\u001b[39;00m mll, output, target, iparams \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlls, outputs, targets, params)\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/mlls/sum_marginal_log_likelihood.py:33\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    outputs: (Iterable[MultivariateNormal]) - the outputs of the latent function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m        (e.g. parameters in case of heteroskedastic likelihoods)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(params) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     sum_mll \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(mll(output, target) \u001b[39mfor\u001b[39;00m mll, output, target \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlls, outputs, targets))\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     sum_mll \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\n\u001b[1;32m     36\u001b[0m         mll(output, target, \u001b[39m*\u001b[39miparams)\n\u001b[1;32m     37\u001b[0m         \u001b[39mfor\u001b[39;00m mll, output, target, iparams \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlls, outputs, targets, params)\n\u001b[1;32m     38\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:62\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     61\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood(function_dist, \u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> 62\u001b[0m res \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mlog_prob(target)\n\u001b[1;32m     63\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     65\u001b[0m \u001b[39m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/distributions/multivariate_normal.py:168\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    161\u001b[0m         covar \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39mrepeat(\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39m(diff_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m covar_size \u001b[39mfor\u001b[39;00m diff_size, covar_size \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(diff\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], padded_batch_shape)),\n\u001b[1;32m    163\u001b[0m             \u001b[39m1\u001b[39m,\n\u001b[1;32m    164\u001b[0m             \u001b[39m1\u001b[39m,\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    167\u001b[0m \u001b[39m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m covar \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39;49mevaluate_kernel()\n\u001b[1;32m    169\u001b[0m inv_quad, logdet \u001b[39m=\u001b[39m covar\u001b[39m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[39m=\u001b[39mdiff\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), logdet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m res \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m([inv_quad, logdet, diff\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpi)])\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/lazy/added_diag_lazy_tensor.py:187\u001b[0m, in \u001b[0;36mAddedDiagLazyTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_kernel\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    177\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m    Overriding this is currently necessary to allow for subclasses of AddedDiagLT to be created. For example,\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m    consider the following:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m    In particular, covar1 would *always* be a standard AddedDiagLazyTensor, but covar2 might be a subtype.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     added_diag_lazy_tsr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentation_tree()(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepresentation())\n\u001b[1;32m    188\u001b[0m     \u001b[39mreturn\u001b[39;00m added_diag_lazy_tsr\u001b[39m.\u001b[39m_lazy_tensor \u001b[39m+\u001b[39m added_diag_lazy_tsr\u001b[39m.\u001b[39m_diag_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/lazy/lazy_tensor.py:1636\u001b[0m, in \u001b[0;36mLazyTensor.representation_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepresentation_tree\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1630\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m \u001b[39m    Returns a :obj:`gpytorch.lazy.LazyTensorRepresentationTree` tree object that recursively encodes the\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[39m    representation of this lazy tensor. In particular, if the definition of this lazy tensor depends on other\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[39m    lazy tensors, the tree is an object that can be used to reconstruct the full structure of this lazy tensor,\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[39m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1636\u001b[0m     \u001b[39mreturn\u001b[39;00m LazyTensorRepresentationTree(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/lazy/lazy_tensor_representation_tree.py:13\u001b[0m, in \u001b[0;36mLazyTensorRepresentationTree.__init__\u001b[0;34m(self, lazy_tsr)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m lazy_tsr\u001b[39m.\u001b[39m_args:\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(arg, \u001b[39m\"\u001b[39m\u001b[39mrepresentation\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m callable(arg\u001b[39m.\u001b[39mrepresentation):  \u001b[39m# Is it a lazy tensor?\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m         representation_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(arg\u001b[39m.\u001b[39;49mrepresentation())\n\u001b[1;32m     14\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren\u001b[39m.\u001b[39mappend((\u001b[39mslice\u001b[39m(counter, counter \u001b[39m+\u001b[39m representation_size, \u001b[39mNone\u001b[39;00m), arg\u001b[39m.\u001b[39mrepresentation_tree()))\n\u001b[1;32m     15\u001b[0m         counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m representation_size\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:381\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrepresentation()\n\u001b[1;32m    378\u001b[0m \u001b[39m# Otherwise, we'll evaluate the kernel (or at least its LazyTensor representation) and use its\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m# representation\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_kernel()\u001b[39m.\u001b[39mrepresentation()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_in_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m _add_to_cache(\u001b[39mself\u001b[39m, cache_name, method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m _get_from_cache(\u001b[39mself\u001b[39m, cache_name, \u001b[39m*\u001b[39margs, kwargs_pkl\u001b[39m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:336\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m     temp_active_dims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims\n\u001b[1;32m    335\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel(\n\u001b[1;32m    337\u001b[0m         x1,\n\u001b[1;32m    338\u001b[0m         x2,\n\u001b[1;32m    339\u001b[0m         diag\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    340\u001b[0m         last_dim_is_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_dim_is_batch,\n\u001b[1;32m    341\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel\u001b[39m.\u001b[39mactive_dims \u001b[39m=\u001b[39m temp_active_dims\n\u001b[1;32m    345\u001b[0m \u001b[39m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/kernels/kernel.py:408\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[1;32m    406\u001b[0m     res \u001b[39m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, last_dim_is_batch\u001b[39m=\u001b[39mlast_dim_is_batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m    407\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     res \u001b[39m=\u001b[39m lazify(\u001b[39msuper\u001b[39;49m(Kernel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x1_, x2_, last_dim_is_batch\u001b[39m=\u001b[39;49mlast_dim_is_batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams))\n\u001b[1;32m    409\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     32\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/kernels/scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[0;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, diag\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m--> 109\u001b[0m     orig_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_kernel\u001b[39m.\u001b[39;49mforward(x1, x2, diag\u001b[39m=\u001b[39;49mdiag, last_dim_is_batch\u001b[39m=\u001b[39;49mlast_dim_is_batch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    110\u001b[0m     outputscales \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputscale\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m last_dim_is_batch:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/kernels/rbf_kernel.py:86\u001b[0m, in \u001b[0;36mRBFKernel.forward\u001b[0;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[1;32m     82\u001b[0m     x2_ \u001b[39m=\u001b[39m x2\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengthscale)\n\u001b[1;32m     83\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovar_dist(\n\u001b[1;32m     84\u001b[0m         x1_, x2_, square_dist\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, diag\u001b[39m=\u001b[39mdiag, dist_postprocess_func\u001b[39m=\u001b[39mpostprocess_rbf, postprocess\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m     85\u001b[0m     )\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m RBFCovariance\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m     87\u001b[0m     x1,\n\u001b[1;32m     88\u001b[0m     x2,\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlengthscale,\n\u001b[1;32m     90\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x1, x2: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcovar_dist(\n\u001b[1;32m     91\u001b[0m         x1, x2, square_dist\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, diag\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dist_postprocess_func\u001b[39m=\u001b[39;49mpostprocess_rbf, postprocess\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m     92\u001b[0m     ),\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlim/lib/python3.10/site-packages/gpytorch/functions/rbf_covariance.py:16\u001b[0m, in \u001b[0;36mRBFCovariance.forward\u001b[0;34m(ctx, x1, x2, lengthscale, sq_dist_func)\u001b[0m\n\u001b[1;32m     14\u001b[0m unitless_sq_dist \u001b[39m=\u001b[39m sq_dist_func(x1_, x2_)\n\u001b[1;32m     15\u001b[0m \u001b[39m# clone because inplace operations will mess with what's saved for backward\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m unitless_sq_dist_ \u001b[39m=\u001b[39m unitless_sq_dist\u001b[39m.\u001b[39;49mclone() \u001b[39mif\u001b[39;00m needs_grad \u001b[39melse\u001b[39;00m unitless_sq_dist\n\u001b[1;32m     17\u001b[0m covar_mat \u001b[39m=\u001b[39m unitless_sq_dist_\u001b[39m.\u001b[39mdiv_(\u001b[39m-\u001b[39m\u001b[39m2.0\u001b[39m)\u001b[39m.\u001b[39mexp_()\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m needs_grad:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.63 GiB (GPU 0; 79.17 GiB total capacity; 4.63 GiB already allocated; 2.31 GiB free; 4.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "training_iterations = 50\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(*model.train_inputs)\n",
    "    loss = -mll(output, model.train_targets)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
