{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utilities.fits import fit\n",
    "# from utilities import plot\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "dist = tfp.distributions\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from skgpytorch.metrics import mean_squared_error, negative_log_predictive_density\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# from datasets.dataset_load import dataset_loader\n",
    "from utilities import plot,fits,gmm,errors,predict,preprocess\n",
    "\n",
    "# device = \"cpu\"\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel, PeriodicKernel, MaternKernel, CosineKernel\n",
    "from skgpytorch.models import SVGPRegressor, SGPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train ={\n",
    "  \n",
    "        3: {\n",
    "          'start_time': \"2011-04-19\" ,\n",
    "          'end_time': \"2011-05-22\"\n",
    "    }\n",
    "     ,\n",
    "          2: {\n",
    "          'start_time': \"2011-04-21\" ,\n",
    "          'end_time': \"2011-05-21\"\n",
    "    }\n",
    " \n",
    "}\n",
    "test = {  1: {\n",
    "          'start_time': \"2011-04-28\" ,\n",
    "          'end_time': \"2011-05-15\"\n",
    "        },\n",
    "     \n",
    "}\n",
    "appliances = [\"Microwave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def dataset_load(appliances, train, test=None):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_train_timestamp = []\n",
    "    n = 9\n",
    "    units_to_pad = n // 2\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    # train\n",
    "    for key, values in train.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp_train = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_train.extend(x)\n",
    "        y_train.extend(y)\n",
    "        x_train_timestamp.extend(torch.Tensor(timestamp_train))\n",
    "\n",
    "\n",
    "    x_train = jnp.array(x_train)\n",
    "    y_train = jnp.array(y_train).reshape(-1, 1)\n",
    "    x_train_timestamp = torch.Tensor(x_train_timestamp).reshape(-1,1)\n",
    "    x_train = scaler_x.fit_transform(x_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    x_train_timestamp = scaler_time.fit_transform(x_train_timestamp)\n",
    "\n",
    "\n",
    "    # test\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    x_test_timestamp = []\n",
    "    for key, values in test.items():\n",
    "        df = pd.read_csv(\n",
    "            f\"datasets/Building{key}_NILM_data_basic.csv\", usecols=[\"Timestamp\", \"main\", appliances[0]])\n",
    "        df[\"date\"] = pd.to_datetime(df[\"Timestamp\"]).dt.date\n",
    "        startDate = datetime.strptime(values[\"start_time\"], \"%Y-%m-%d\").date()\n",
    "        endDate = datetime.strptime(values[\"end_time\"], \"%Y-%m-%d\").date()\n",
    "\n",
    "        if startDate > endDate:\n",
    "            raise \"Start Date must be smaller than Enddate.\"\n",
    "\n",
    "        df = df[(df[\"date\"] >= startDate) & (df[\"date\"] <= endDate)]\n",
    "        df.dropna(inplace=True)\n",
    "        x = df[\"main\"].values\n",
    "        y = df[appliances[0]].values\n",
    "        timestamp = (pd.to_datetime(df[\"Timestamp\"]).astype(int)/ 10**18).values\n",
    "        x = jnp.pad(x, (units_to_pad, units_to_pad),\n",
    "                    'constant', constant_values=(0, 0))\n",
    "        x = jnp.array([x[i: i + n] for i in range(len(x) - n + 1)])\n",
    "        x_test.extend(x)\n",
    "        y_test.extend(y)\n",
    "        x_test_timestamp.extend(timestamp)\n",
    "\n",
    "    x_test = jnp.array(x_test)\n",
    "    y_test = jnp.array(y_test).reshape(-1, 1)\n",
    "    x_test_timestamp = torch.Tensor(x_test_timestamp).reshape(-1,1)\n",
    "\n",
    "    x_test = scaler_x.transform(x_test)\n",
    "    x_test_timestamp = scaler_time.transform(x_test_timestamp)\n",
    "#     y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    x_train = jnp.array(x_train).reshape(x_train.shape[0], n)\n",
    "    y_train = jnp.array(y_train)\n",
    "    x_train_timestamp = torch.Tensor(x_train_timestamp).reshape(x_train_timestamp.shape[0], 1)\n",
    "    x_test = jnp.array(x_test).reshape(x_test.shape[0], n)\n",
    "    y_test = jnp.array(y_test)\n",
    "    x_test_timestamp = torch.Tensor(x_test_timestamp).reshape(x_test_timestamp.shape[0], 1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_train_timestamp, x_test_timestamp, scaler_x, scaler_y, scaler_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_train_timstamp, x_test_timestamp, scaler_x, scaler_y,scaler_time = dataset_load(appliances, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = 53456\n",
    "n = 9\n",
    "# x_train_full = jnp.concatenate((jnp.array(x_train.reshape(-1,n)), jnp.array(np.array(x_train_timstamp))), axis=1)\n",
    "# x_test_full = jnp.concatenate((jnp.array(x_test.reshape(-1,n)), jnp.array(np.array(x_test_timestamp))), axis=1)\n",
    "# x_train_full.shape, x_test_full.shape\n",
    "\n",
    "x = torch.Tensor(np.array(x_train))\n",
    "y = torch.Tensor(np.array(y_train)).reshape(-1)\n",
    "xt = torch.Tensor(np.array(x_test))\n",
    "yt = torch.Tensor(np.array(y_test)).reshape(-1)\n",
    "\n",
    "# if x.shape[0]>indexes:\n",
    "#   x1 = x[:indexes]\n",
    "#   y1 = y[:indexes]\n",
    "\n",
    "# xt1= x[indexes:]\n",
    "# yt1 = y[indexes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y.inverse_transform(yt.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = x.size(-1)\n",
    "class LargeFeatureExtractor1(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor1, self).__init__()\n",
    "        self.add_module('lstm', torch.nn.LSTM(data_dim, 256,4))\n",
    "        # self.add_module('linear1', torch.nn.Linear(data_dim, 250))\n",
    "        # self.add_module('relu1', torch.nn.ReLU())\n",
    "        # self.add_module('linear2', torch.nn.Linear(250, 50))     \n",
    "        # self.add_module('relu2', torch.nn.ReLU())                  \n",
    "        # self.add_module('linear3', torch.nn.Linear(50, 9))       \n",
    "        # self.add_module('relu3', torch.nn.ReLU())   \n",
    "        # self.add_module('linear4', torch.nn.Linear(250, 100))       \n",
    "        # self.add_module('relu4', torch.nn.ReLU())                         \n",
    "        # self.add_module('linear5', torch.nn.Linear(100, 9))\n",
    "\n",
    "class LargeFeatureExtractor2(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor2, self).__init__()\n",
    "        # self.add_module('lstm', torch.nn.LSTM(data_dim, 500,2))\n",
    "        # self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        # self.add_module('relu1', torch.nn.ReLU())\n",
    "        # self.add_module('linear2', torch.nn.Linear(1000, 500))     \n",
    "        # self.add_module('relu2', torch.nn.ReLU())                  \n",
    "        self.add_module('linear1', torch.nn.Linear(256, 64))       \n",
    "        self.add_module('relu1', torch.nn.ReLU())   \n",
    "        self.add_module('linear2', torch.nn.Linear(64, 9))                          \n",
    "        # self.add_module('linear1', torch.nn.Linear(200, 99))  \n",
    "\n",
    "feature_extractor1 = LargeFeatureExtractor1().cuda()\n",
    "feature_extractor2 = LargeFeatureExtractor2().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.base_covar_module =  gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=9)) \n",
    "            #+ gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=1, active_dims=(49)))*gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel(ard_num_dims=1, active_dims=(49)))\n",
    "            # self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            #     gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "            #     num_dims=2, grid_size=100\n",
    "            # )\n",
    "            self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[np.arange(0, train_x.shape[0], 25)], likelihood=likelihood)\n",
    "            print(self.covar_module)\n",
    "            self.feature_extractor1 = feature_extractor1\n",
    "            self.feature_extractor2 = feature_extractor2\n",
    "\n",
    "            # This module will scale the NN features so that they're nice values\n",
    "            # self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(-1., 1.)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            projected_x, (hn,cn) = self.feature_extractor1(x)\n",
    "            print(projected_x.shape, hn.shape,cn.shape)\n",
    "            projected_x = self.feature_extractor2(projected_x)\n",
    "            # print(projected_x.shape, hn.shape,cn.shape)\n",
    "            # projected_x = self.scale_to_bounds(projected_x)  # Make the NN values \"nice\"\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "            # print(projected_x.dtype)\n",
    "            mean_x = self.mean_module(projected_x) #projected_\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(x, y, likelihood)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "training_iterations = 200\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "loss_arr =[]\n",
    "def train():\n",
    "    iterator = tqdm.notebook.tqdm(range(training_iterations))\n",
    "    for i in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(x.cuda())\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, y.cuda())\n",
    "        # loss_arr.append(loss.cpu())\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(xt.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(torch.tensor(scaler_y.inverse_transform(preds.mean.cpu().reshape(-1,1)).squeeze()) - yt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = scaler_y.inverse_transform(preds.mean.cpu().reshape(-1,1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 300\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(jnp.arange(idx),yt[:idx], label = \"Micro\")\n",
    "plt.plot(jnp.arange(idx), y_mean[:idx], label = \"Predicted\")\n",
    "plt.legend(bbox_to_anchor=(1, 1),fontsize=20)\n",
    "sns.despine()\n",
    "# plt.savefig(\"./Results/Refrigerator_building5.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test MAE: 16.795347555539852 Building2 \n",
    "#Test MAE: 36.80607449241171 Building1\n",
    "#Test MAE: 26.90407449241171 Building3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a19952a8cb0d513e360355f3718fc7b5b0ccef7313ddd97e7b7ab66b1ecfbb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
